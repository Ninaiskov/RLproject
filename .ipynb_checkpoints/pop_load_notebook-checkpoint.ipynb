{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"getting-started-ppo.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1rIRIJLBwhn36KNr8TkNXoXTFokv8T-V6\n",
    "\n",
    "# Getting started with PPO and ProcGen\n",
    "\n",
    "Here's a bit of code that should help you get started on your projects.\n",
    "\n",
    "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details.\n",
    "\"\"\"\n",
    "\n",
    "#!pip install procgen\n",
    "#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
    "\n",
    "\"\"\"Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation.\"\"\"\n",
    "\n",
    "env_name = 'climber'\n",
    "#exp_name = input('Insert experiment name: ')\n",
    "exp_name = 'natureDQN_nobackground'\n",
    "# Hyperparameters\n",
    "total_steps = 25e6\n",
    "num_envs = 64 # 32\n",
    "num_levels = 200 #10\n",
    "num_steps = 256 #256\n",
    "num_epochs = 3\n",
    "batch_size = 512 #512\n",
    "eps = .2\n",
    "grad_eps = .5\n",
    "value_coef = .5\n",
    "entropy_coef = .01\n",
    "\n",
    "\"\"\"Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureCNN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM).\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import make_env, Storage, orthogonal_init\n",
    "import procgen\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels, feature_dim):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "        Flatten(),\n",
    "        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
    "    )\n",
    "    self.apply(orthogonal_init)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "  def __init__(self, encoder, feature_dim, num_actions):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
    "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
    "\n",
    "  def act(self, x):\n",
    "    with torch.no_grad():\n",
    "      x = x.cuda().contiguous()\n",
    "      dist, value = self.forward(x)\n",
    "      action = dist.sample()\n",
    "      log_prob = dist.log_prob(action)\n",
    "    \n",
    "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    logits = self.policy(x)\n",
    "    value = self.value(x).squeeze(1)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    return dist, value\n",
    "\n",
    "\n",
    "# Define environment\n",
    "# check the utils.py file for info on arguments\n",
    "env = make_env(num_envs, env_name, num_levels=num_levels, use_backgrounds=False)\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Action space:', env.action_space.n)\n",
    "\n",
    "# Define network\n",
    "in_channels = env.observation_space.shape[0] # shape of state\n",
    "feature_dim = 512 # arbitrary chosen\n",
    "num_actions = env.action_space.n # number of possible actions (for climber: left, right, jump)\n",
    "\n",
    "encoder = Encoder(in_channels, feature_dim)\n",
    "policy = Policy(encoder, feature_dim, num_actions)\n",
    "policy.cuda()\n",
    "\n",
    "checkpoint_name = 'checkpoint_impala_impalaExtra_nobg_k4.pth'\n",
    "\n",
    "\n",
    "policy.load_state_dict(torch.load( 'checkpoint_'+exp_name+'.pt'))\n",
    "\n",
    "\"\"\"# Notes:\n",
    "\n",
    "PPO (proximal policy optimization) is based on TRPO (trust region policy optimization). TRPO add a KL constraint in order to make sure that the updated policy doesn't move to far away from the old policy (we want to stay in the trusted region, i.e. the region where we know everything works fine). However, the KL constraint increases the complexity of optimization problem and can lead to undesireable training behavior. Instead we wish to implement the constraint directly into our optimization objective function. This is excactly what PPO does. \n",
    "\n",
    "First, we define a $r_t(\\theta)$ as the probability ratio between the outputs from the new updated policy (being learned) and outputs from the old policy (baseline, from earlier experiece), so given a sequence of sample actions and states, the probability ratio will be: \n",
    "- larger than 1 if the action is more likely in the new than old policy (numerator > denominator).\n",
    "- between 0 and 1 if the action is less likely in the new than old policy (numerator < denominator).\n",
    "\n",
    "We can then multiply the probability ratio, $r_t(\\theta)$, with the advantage function, $A_t$, to get the normal policy gradients (PG) objective.\n",
    "- The advantage is an estimate for how good an action is compared to the average action for a specific state, i.e. a relative action-value: A=Q-V, where Q is the absolute action-value/quality function and V is the state-value function.\n",
    "\n",
    "The purpose of the clipping function is to truncate the normal policy gradients objective. \n",
    "\n",
    "So the $L_{CLIP}$ compares the normal policy gradients objective and a clipped version of it and takes the minimum (smallest one). Then we apply the expectation operator, i.e. we compute the mean of the batches of trajectories.\n",
    "\n",
    "The advantage function is noisy and we don't want to change our policy drastically based on a single estimate. This is where the clipping function comes in handy. We remember the adavantage function, $A_t$, can be both postive and negative and this changes the effect of the min-operator. We have 3 cases:\n",
    "- If action is good (A > 0) and it became more probably than before (r > 1), then we don't want to keep increasing likelihood too much, so we clip it (plateu phase)\n",
    "- If the action is bad (A < 0) and it became less probable (r < 1), then we don't want to keep reducing likelihood too much, so we clip it.\n",
    "- If the action is bad (A < 0) and it became more probable (r > 1), then we want to undo our last update. Note: this is the only region where the normal PG objective < clipped PG objective.\n",
    "\n",
    "The final training objective in PPO (for training an agent) is the sum of $L_{CLIP}$ and two additional terms: $L_{VF}$ and $S$.\n",
    "- $L_{VF}$ is the value function objective, which is in charge of updating the baseline network: how good is it to be in this state / what is the expected average amount of discounted reward?\n",
    "- $S$ is the entropy term, which is in charge of making sure the the agent does enough exploration durinng training (we want it to act a bit randomly until the other parts of the objective starts dominating). Entropy is the average amount of bits that is needed to represent its outcome - it is a measure of how unpredictable an outcome of this variable really is. Maximing the entropy will therefore force the distribution to have a wide spread over all possible option resulting in the most unpredictable outcome. The PPO Head outputs the parameters of a Gaussian distribution for each possible action. When running the agent in training mode, the policy samples from these distributions to get continous output value for each action.\n",
    "\n",
    "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view.\n",
    "\"\"\"\n",
    "\n",
    "import imageio\n",
    "\n",
    "# Make evaluation environment\n",
    "eval_env = make_env(num_envs, env_name, start_level=num_levels, num_levels=num_levels, use_backgrounds=False)\n",
    "obs = eval_env.reset()\n",
    "\n",
    "frames = []\n",
    "total_reward = []\n",
    "\n",
    "# Evaluate policy\n",
    "policy.eval()\n",
    "for _ in range(25*60):\n",
    "\n",
    "  # Use policy\n",
    "  action, log_prob, value = policy.act(obs)\n",
    "\n",
    "  # Take step in environment\n",
    "  obs, reward, done, info = eval_env.step(action)\n",
    "  total_reward.append(torch.Tensor(reward))\n",
    "\n",
    "  # Render environment and store\n",
    "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
    "  frames.append(frame)\n",
    "\n",
    "# Calculate average return\n",
    "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
    "#norm_reward = ((torch.stack(total_reward)- torch.stack(total_reward).min(0))/(torch.stack(total_reward).min(0)+torch.stack(total_reward).max(0))).mean(0)\n",
    "print('Average return:', total_reward)\n",
    "#print('Normalized average return:', norm_reward)\n",
    "\n",
    "# Save frames as video\n",
    "frames = torch.stack(frames)\n",
    "imageio.mimsave('vid_'+exp_name+'.mp4', frames, fps=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
