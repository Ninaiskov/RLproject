# -*- coding: utf-8 -*-
"""getting-started-ppo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rIRIJLBwhn36KNr8TkNXoXTFokv8T-V6

# Getting started with PPO and ProcGen

Here's a bit of code that should help you get started on your projects.

The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details.
"""

#!pip install procgen
#!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import make_env, Storage, orthogonal_init
import numpy as np

"""Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."""

env_name = 'climber'
k = 2 # Impala scaling parameter
exp_name = 'impalaExtra_nobg_k'+str(k)+'_continued'
use_backgrounds = False
continue_from_checkpoint = True
checkpoint_name = 'checkpoint_impalaExtra_nobg_k'+str(k)+'.pth'

# Hyperparameters (same as in article, optimal for PPO with adam optimizer)
total_steps = 25e6 #8e6 # total timesteps
num_envs = 64 #32 rollouts?
num_levels = 200 #10
num_steps = 256 # timesteps per rollout
num_epochs = 3 # epochs per rollout
batch_size = 8 #512 #
eps = .2 # PPO clip range
grad_eps = .5 # not specified in article?
value_coef = .5 # not specified in article?
entropy_coef = .01 # entropy bonus
lr = 5e-4*1/np.sqrt(k)

"""Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."""

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)


# Impala-CNN: https://medium.com/aureliantactics/custom-models-with-baselines-impala-cnn-cnns-with-features-and-contra-3-hard-mode-811dbdf2dff9
class Encoder(nn.Module):
  def __init__(self, in_channels, feature_dim):
    super().__init__()
    self.conv1 = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=16*k, kernel_size=3, stride=1), nn.LeakyReLU()
    )
    self.conv2 = nn.Sequential(
        nn.Conv2d(in_channels=16*k, out_channels=32*k, kernel_size=3, stride=1), nn.LeakyReLU()
    )
    self.conv3 = nn.Sequential(
        nn.Conv2d(in_channels=32*k, out_channels=32*k, kernel_size=3, stride=1), nn.LeakyReLU()
    )
    self.resblock1 = nn.Sequential(
        nn.Conv2d(in_channels=16*k, out_channels=16*k, kernel_size=3, stride=1, padding = 1), nn.LeakyReLU(),
        nn.Conv2d(in_channels=16*k, out_channels=16*k, kernel_size=3, stride=1, padding = 1), nn.LeakyReLU()
    )
    self.resblock2 = nn.Sequential(
        nn.Conv2d(in_channels=32*k, out_channels=32*k, kernel_size=3, stride=1, padding = 1), nn.LeakyReLU(),
        nn.Conv2d(in_channels=32*k, out_channels=32*k, kernel_size=3, stride=1, padding = 1), nn.LeakyReLU()
    )
    self.maxpooling = nn.MaxPool2d(kernel_size = 3, stride = 2)
    self.linear = nn.Sequential(
        nn.Linear(in_features = k*32*5*5, out_features = feature_dim)
    )
    self.flat = nn.Flatten()
    self.apply(orthogonal_init)


  def forward(self, x):
    x = self.conv1(x)
    x = self.maxpooling(x)
    x_res = x.clone()
    x = self.resblock1(x) + x_res
    x_res = x.clone()
    x = self.resblock1(x) + x_res
    x = self.conv2(x)
    x = self.maxpooling(x)
    x_res = x.clone()
    x = self.resblock2(x) + x_res
    x_res = x.clone()
    x = self.resblock2(x) + x_res
    x = self.conv3(x)
    x = self.maxpooling(x)
    x_res = x.clone()
    x = self.resblock2(x) + x_res
    x_res = x.clone()
    x = self.resblock2(x) + x_res
    x = self.flat(x)
    x = self.linear(x)
    return x


class Policy(nn.Module):
  def __init__(self, encoder, feature_dim, num_actions):
    super().__init__()
    self.encoder = encoder
    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)
    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)

  def act(self, x):
    with torch.no_grad():
      x = x.cuda().contiguous()
      dist, value = self.forward(x)
      action = dist.sample()
      log_prob = dist.log_prob(action)
    
    return action.cpu(), log_prob.cpu(), value.cpu()

  def forward(self, x):
    x = self.encoder(x)
    logits = self.policy(x)
    value = self.value(x).squeeze(1)
    dist = torch.distributions.Categorical(logits=logits)

    return dist, value


# Define environment
# check the utils.py file for info on arguments
env = make_env(num_envs, env_name, start_level = 0, num_levels=num_levels, distribution_mode='easy', use_backgrounds=use_backgrounds)
print('Observation space:', env.observation_space)
print('Action space:', env.action_space.n)
envTest = make_env(num_envs, env_name, start_level = num_levels, num_levels=num_levels, distribution_mode = 'easy', use_backgrounds=use_backgrounds)

# Define network
in_channels = env.observation_space.shape[0] # shape of state
feature_dim = 256 # arbitrary chosen
num_actions = env.action_space.n # number of possible actions (for climber: left, right, jump)

encoder = Encoder(in_channels, feature_dim)
policy = Policy(encoder, feature_dim, num_actions)
policy.cuda()

# Define optimizer
# these are reasonable values but probably not optimal
optimizer = torch.optim.Adam(policy.parameters(), lr=lr, eps=1e-5, weight_decay=1e-4)
# note: weight_decay is equal to L2 regularization

# Load checkpoint
if continue_from_checkpoint == True:
  checkpoint = torch.load(checkpoint_name)
  policy.load_state_dict(checkpoint['model'])
  optimizer.load_state_dict(checkpoint['optimizer'])
else: pass

# Define temporary storage
# we use this to collect transitions during each iteration
storage = Storage(
    env.observation_space.shape,
    num_steps,
    num_envs
)

# Run training
training_rewards=[]
test_rewards=[]
step_list=[]
obs = env.reset()
obsTest = envTest.reset()
step = 0
while step < total_steps:

  # Use policy to collect data for num_steps steps
  policy.eval()
  for _ in range(num_steps): # for each step we update the baseline (old) policy
    # Use policy
    action, log_prob, value = policy.act(obs)
    
    # Take step in environment
    next_obs, reward, done, info = env.step(action)

    # Store data
    storage.store(obs, action, reward, done, info, log_prob, value)
    
    # Update current observation
    obs = next_obs

  # Add the last observation to collected data
  _, _, value = policy.act(obs)
  storage.store_last(obs, value)

  # Compute return and advantage
  storage.compute_return_advantage()

  # Optimize policy
  policy.train()
  for epoch in range(num_epochs): # for each epoch we update the new policy

    # Iterate over batches of transitions
    generator = storage.get_generator(batch_size)
    for batch in generator: 
      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch

      # Get current policy outputs
      new_dist, new_value = policy(b_obs)
      new_log_prob = new_dist.log_prob(b_action)

      # Clipped policy objective
      ratio = torch.exp(new_log_prob - b_log_prob) # exp(new policy - old policy)  
      clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps)
      policy_reward = -torch.min(ratio * b_advantage, clipped_ratio * b_advantage)
      pi_loss = policy_reward.mean()

      # Clipped value function objective
      clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps)
      vf_loss = torch.max((new_value - b_returns) ** 2, (clipped_value - b_returns) ** 2)
      value_loss = value_coef * vf_loss.mean()

      # Entropy loss
      entropy_loss = new_dist.entropy().mean()

      # Backpropagate losses (see https://www.youtube.com/watch?v=5P7I-xPq8u8 at 15:05)
      # entropy controlls the distrubtion variance for exploitation
      loss = value_loss + pi_loss - entropy_coef * entropy_loss
      loss.backward()

      # Clip gradients
      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)

      # Update policy
      optimizer.step()
      optimizer.zero_grad()


  # Evaluate policy (test)
  total_reward=[]
  policy.eval()
  for _ in range(num_steps): # for each step we update the baseline (old) policy
    # Use policy
    action, _, _ = policy.act(obsTest)
    
    # Take step in environment
    next_obsTest, reward, done, info = envTest.step(action)
    total_reward.append(torch.Tensor(reward))
    
    # Update current observation
    obsTest = next_obs

  # Add the last observation to collected data
  test_rewards.append(torch.stack(total_reward).sum(0).mean(0))


  # Update stats
  step += num_envs * num_steps
  print(f'Step: {step}\tMean reward: {storage.get_reward()}')

  # bookkeeping
  training_rewards.append(storage.get_reward())
  step_list.append(step)

  if step >= total_steps:
    print('Completed training!')
    checkpoint = { 
    'step': step,
    'step list': step_list,
    'model': policy.state_dict(),
    'optimizer': optimizer.state_dict(),
    #'lr_sched': lr_sched,
    'training rewards': training_rewards,
    'test rewards': test_rewards}
    torch.save(checkpoint, 'checkpoint_completed_'+exp_name+'.pth')
  else:
    checkpoint = { 
    'step': step,
    'step list': step_list,
    'model': policy.state_dict(),
    'optimizer': optimizer.state_dict(),
    #'lr_sched': lr_sched,
    'training rewards': training_rewards,
    'test rewards': test_rewards}
    torch.save(checkpoint, 'checkpoint_'+exp_name+'.pth')

print('Completed training!')
#torch.save(policy.state_dict(), 'checkpoint_'+exp_name+'.pt')

'''
# Below cell can be used for policy evaluation and saves an episode to mp4 for you to view.

import imageio

# Make evaluation environment
eval_env = make_env(num_envs, env_name, start_level=num_levels, num_levels=num_levels, distribution_mode='easy', use_backgrounds=False)
obs = eval_env.reset()

frames = []
total_reward = []

# Evaluate policy
policy.eval()
for _ in range(512):

  # Use policy
  action, log_prob, value = policy.act(obs)

  # Take step in environment
  obs, reward, done, info = eval_env.step(action)
  total_reward.append(torch.Tensor(reward))

  # Render environment and store
  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()
  frames.append(frame)

# Calculate average return
total_reward = torch.stack(total_reward).sum(0).mean(0)
print('Average return:', total_reward)

# Save frames as video
frames = torch.stack(frames)
imageio.mimsave('vid_'+exp_name+'.mp4', frames, fps=25)
'''